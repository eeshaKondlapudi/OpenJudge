"""
Financial Deep Research Report Completeness Grader

This module provides a grader for evaluating the completeness of financial research
agent reports - whether the report fully covers all key points and requirement dimensions.
"""

import textwrap
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from loguru import logger
from pydantic import BaseModel, Field

from rm_gallery.core.graders.base_grader import GraderMode, GraderScore
from rm_gallery.core.graders.llm_grader import LLMGrader
from rm_gallery.core.graders.schema import GraderError
from rm_gallery.core.models.base_chat_model import BaseChatModel
from rm_gallery.core.models.schema.oai.message import ChatMessage
from rm_gallery.core.models.schema.prompt_template import LanguageEnum, PromptTemplate

# pylint: disable=line-too-long

# Chinese Prompt
FINANCIAL_COMPLETENESS_PROMPT_ZH = """# 任务
你是一个顶级的金融研究报告评估专家,精通金融学、经济学、公司财务、公司金融、投资分析、证券分析、市场研究、行情解读、行情分析、金融科技新闻咨询解读等金融领域专业知识。你的任务是站在用户视角,评估金融研究Agent生成的研究报告的**完整性**。请仔细阅读以下信息,并按照给定的评分标准进行评估。

# 待评估信息
1. 这是用户的问题:<user_question>{query}</user_question>
2. 这是金融研究Agent生成的研究报告:<analyst_answer>{answer}</analyst_answer>
3. 这是报告生成的日期:<date>{chat_date}</date>

# 评估标准

## **底线问题标准(直接得1分)**
如果报告满足以下任意一条,直接得1分:
1. 报告内容违反法律、常识和基本金融知识,属于致命错误!!!
2. **严格要求报告的核心分析对象与问题主体一致**:若报告的核心分析对象和<user_question>询问的主体不一致,属于致命错误!!!
3. **严格要求报告内容前后一致**:若报告的关键点、关键指标、核心结论、观点、逻辑等内容存在前后矛盾和不一致情况,会给用户带来困惑,属于致命错误!!!
4. **数据与事实真实性**: 报告中引用的关键数据、事实、事件或引文必须是真实可查的。若出现明显的事实性错误或数据捏造（Hallucination）,属于致命错误!!!

## 完整性评分 (completeness_score)
评估报告是否完整覆盖用户问题中的所有关键点和需求维度（如标的、时间范围、对比对象、核心指标等）:

- **5分**: 完全完整,全面覆盖所有关键点和需求维度,无遗漏
- **4分**: 高度完整,覆盖绝大部分关键点,可能有极少量非核心维度未涉及
- **3分**: 基本完整,覆盖主要关键点,但有部分重要维度缺失或分析不足
- **2分**: 完整性不足,多个关键点缺失,信息覆盖有明显缺口
- **1分**: 严重不完整,大量关键点缺失或触犯底线问题标准

**注意**: 若关键信息不足,应在报告中明确指出信息缺口并说明所做的合理假设,而不是直接编造结论。

# 评分流程
1. 理解<user_question>,并将其归类到一个或多个意图场景类别,以便后续评估参考。场景类别包括但不限于:行业研究、事件解读、个股分析、宏观分析、股票检索、其他等
2. 梳理<user_question>中的所有关键点和需求维度,以及金融研究Agent生成的报告<analyst_answer>的覆盖情况
3. **首先判断是否触犯底线问题标准**,如果触犯任意一条,直接得1分
4. 如果未触犯底线问题标准,根据完整性评分标准进行评分(1-5分)

# 输出要求

请以JSON格式输出评估结果（分数为1-5的整数）：
{{
    "score": <int (1-5)>,
    "reason": "<完整性评估理由，需要说明是否触犯底线问题标准，以及具体的完整性分析，包括覆盖和缺失的关键点>"
}}

JSON:
"""

# English Prompt
FINANCIAL_COMPLETENESS_PROMPT_EN = """# Task
You are a top-tier financial research report evaluation expert, proficient in finance, economics, corporate finance, investment analysis, securities analysis, market research, market interpretation, and financial technology news analysis. Your task is to evaluate the **completeness** of research reports generated by financial research agents from the user's perspective. Please carefully read the following information and evaluate according to the given scoring criteria.

# Information to Evaluate
1. User's question: <user_question>{query}</user_question>
2. Research report generated by financial research agent: <analyst_answer>{answer}</analyst_answer>
3. Report generation date: <date>{chat_date}</date>

# Evaluation Criteria

## **Bottom-line Criteria (Direct score of 1)**
If the report meets any of the following conditions, it gets a direct score of 1:
1. Report content violates laws, common sense, and basic financial knowledge - a fatal error!!!
2. **Strict requirement for consistency between report's core analysis object and question subject**: If the report's core analysis object is inconsistent with the subject in <user_question>, it's a fatal error!!!
3. **Strict requirement for internal consistency**: If there are contradictions in key points, key indicators, core conclusions, viewpoints, or logic, causing user confusion, it's a fatal error!!!
4. **Data and factual authenticity**: Key data, facts, events, or citations in the report must be verifiable. Obvious factual errors or data fabrication (Hallucination) is a fatal error!!!

## Completeness Score (completeness_score)
Evaluate whether the report fully covers all key points and requirement dimensions in the user question (such as targets, time ranges, comparison objects, core indicators, etc.):

- **5**: Completely complete, comprehensively covers all key points and requirement dimensions, no omissions
- **4**: Highly complete, covers vast majority of key points, may have minimal non-core dimensions not covered
- **3**: Basically complete, covers main key points but some important dimensions are missing or insufficiently analyzed
- **2**: Insufficient completeness, multiple key points missing, obvious gaps in information coverage
- **1**: Seriously incomplete, numerous key points missing or violates bottom-line criteria

**Note**: If key information is insufficient, the report should explicitly point out information gaps and explain reasonable assumptions made, rather than fabricating conclusions.

# Evaluation Process
1. Understand <user_question> and categorize it into one or more intent scenario categories for subsequent evaluation reference. Categories include but are not limited to: industry research, event interpretation, individual stock analysis, macroeconomic analysis, stock screening, others, etc.
2. Sort out all key points and requirement dimensions in <user_question>, and the coverage in the report <analyst_answer> generated by financial research agent
3. **First determine if any bottom-line criteria is violated**, if any is violated, directly score 1
4. If no bottom-line criteria is violated, score according to completeness scoring criteria (1-5)

# Output Requirement

Please output your evaluation in JSON format (score is an integer from 1-5):
{{
    "score": <int (1-5)>,
    "reason": "<Completeness evaluation reasoning, need to explain whether bottom-line criteria is violated and specific completeness analysis, including covered and missing key points>"
}}

JSON:
"""

# Build default template from prompts
DEFAULT_FINANCIAL_COMPLETENESS_TEMPLATE = PromptTemplate(
    messages={
        LanguageEnum.EN: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_COMPLETENESS_PROMPT_EN),
            ),
        ],
        LanguageEnum.ZH: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_COMPLETENESS_PROMPT_ZH),
            ),
        ],
    },
)


# Pydantic models for structured LLM output
class FinancialCompletenessEvaluationOutput(BaseModel):
    """Structured output model for financial completeness evaluation LLM response."""

    score: int = Field(ge=1, le=5, description="完整性得分 (1-5)")
    reason: str = Field(default="", description="完整性判断原因")


def _normalize_score(score: Union[int, float]) -> float:
    """
    Normalize a 1-5 integer score to 0-1 continuous scale.

    Mapping:
    - 1 -> 0.0
    - 2 -> 0.25
    - 3 -> 0.5
    - 4 -> 0.75
    - 5 -> 1.0

    Args:
        score: Integer score from 1 to 5

    Returns:
        float: Normalized score from 0.0 to 1.0
    """
    # Clamp score to valid range [1, 5]
    score = max(1, min(5, float(score)))
    # Normalize: (score - 1) / 4
    return (score - 1) / 4.0


class FinancialReportCompletenessGrader(LLMGrader):
    """
    Financial deep research report completeness evaluation grader.

    Evaluates whether a financial research agent's report fully covers all
    key points and requirement dimensions in the user's question.

    This grader first checks for bottom-line criteria violations, then evaluates
    completeness on a 1-5 scale, which is normalized to 0-1 for the final score.

    Attributes:
        name: Grader name
        model: ChatModelBase instance for evaluation
        language: Language for evaluation prompts

    Example:
        >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
        >>> api = OpenAIChatModel(api_key="...", model="gpt-4o")
        >>> grader = FinancialReportCompletenessGrader(model=api)
        >>> result = await grader.aevaluate(
        ...     messages=[
        ...         {"role": "user", "content": "分析某公司财报"},
        ...         {"role": "assistant", "content": "详细的财务分析报告..."}
        ...     ],
        ...     chat_date="2024-01-01"
        ... )
        >>> print(f"Score: {result.score}")  # normalized 0-1 score
    """

    def __init__(
        self,
        model: Union[BaseChatModel, dict],
        template: Optional[PromptTemplate] = DEFAULT_FINANCIAL_COMPLETENESS_TEMPLATE,
        language: LanguageEnum = LanguageEnum.ZH,
    ):
        """
        Initialize the FinancialCompletenessGrader.

        Args:
            model (Union[BaseChatModel, dict]): The chat model to use for evaluation.
                Can be either a BaseChatModel instance or a dictionary configuration.
            template (Optional[PromptTemplate]): The prompt template for completeness evaluation.
                Defaults to DEFAULT_FINANCIAL_COMPLETENESS_TEMPLATE.
            language (LanguageEnum): Language for the evaluation prompt.
                Defaults to LanguageEnum.ZH (Chinese).

        Example:
            >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
            >>> model = OpenAIChatModel(api_key="...", model="gpt-4o")
            >>> grader = FinancialCompletenessGrader(model=model)
        """
        super().__init__(
            name="financial_report_completeness",
            mode=GraderMode.POINTWISE,
            description="Financial deep research report completeness evaluation",
            model=model,
            template=template,
            language=language,
            structured_model=FinancialCompletenessEvaluationOutput,
        )

    def _extract_query_and_answer_from_messages(
        self,
        messages: List[Dict[str, Any]],
    ) -> tuple[str, str]:
        """
        Extract user query and answer from messages.

        Args:
            messages: List of message dicts (standard format).

        Returns:
            Tuple of (query, answer)
        """
        # Filter out system messages and unwrap nested structure
        messages = [msg.get("message", msg) for msg in messages]
        non_system_messages = [msg for msg in messages if msg.get("role", "") != "system"]

        if not non_system_messages:
            return "", ""

        # Extract user query (first non-system user message)
        query = ""
        if non_system_messages[0].get("role", "") == "user":
            query = non_system_messages[0].get("content", "")

        # Extract answer (last assistant message content)
        answer = ""
        for msg in reversed(non_system_messages):
            if msg.get("role", "") == "assistant" and msg.get("content", ""):
                answer = msg.get("content", "")
                break

        return query, answer

    async def aevaluate(
        self,
        messages: List[Dict[str, Any]],
        chat_date: Optional[str] = None,
    ) -> GraderScore:
        """
        Evaluate financial research report completeness.

        The evaluation first checks for bottom-line criteria violations, then scores
        completeness on a 1-5 scale, which is normalized to 0-1 for the final score.

        Args:
            messages: List of messages (standard format, including system, user, assistant)
                The "message" key for message can be optional.

            chat_date (Optional[str]): Date of report generation. If not provided,
                uses current system date (YYYY-MM-DD format)

        Returns:
            GraderScore: Completeness evaluation score for the report (normalized 0.0-1.0)
                - score: Normalized completeness score (0.0-1.0)
                - reason: Completeness evaluation reasoning
                - metadata: Contains raw_score (1-5)

        Example:
            >>> result = await grader.aevaluate(
            ...     messages=[
            ...         {"role": "user", "content": "分析某公司的财务状况"},
            ...         {"role": "assistant", "content": "详细的财务分析报告..."}
            ...     ],
            ...     chat_date="2024-01-01"
            ... )
            >>> print(f"Completeness Score: {result.score}")
        """
        # Extract query and answer from messages
        query, answer = self._extract_query_and_answer_from_messages(messages)

        # Use provided chat_date if available, otherwise use current system date
        if chat_date is None:
            chat_date = datetime.now().strftime("%Y-%m-%d")

        if not query or not answer:
            logger.warning("Empty query or answer, returning error")
            return GraderError(name=self.name, error="Empty query or answer")

        try:
            # Call parent evaluation with formatted parameters
            result = await super().aevaluate(
                query=query,
                answer=answer,
                chat_date=chat_date,
            )

            # Normalize score from 1-5 to 0-1
            normalized_score = _normalize_score(result.score)

            return GraderScore(
                name=self.name,
                score=normalized_score,
                reason=result.reason,
                metadata=result.metadata,
            )

        except Exception as e:
            logger.error(f"Error evaluating {self.name}: {e}")
            return GraderError(name=self.name, error=str(e))
