"""
Financial Deep Research Report Data Support Grader

This module provides a grader for evaluating the data support of financial research
agent reports - whether conclusions have sufficient data, facts, or case support.
"""

import textwrap
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from loguru import logger
from pydantic import BaseModel, Field

from rm_gallery.core.graders.base_grader import GraderMode, GraderScore
from rm_gallery.core.graders.llm_grader import LLMGrader
from rm_gallery.core.graders.schema import GraderError
from rm_gallery.core.models.base_chat_model import BaseChatModel
from rm_gallery.core.models.schema.oai.message import ChatMessage
from rm_gallery.core.models.schema.prompt_template import LanguageEnum, PromptTemplate

# pylint: disable=line-too-long

# Chinese Prompt
FINANCIAL_DATA_SUPPORT_PROMPT_ZH = """# 任务
你是一个顶级的金融研究报告评估专家,精通金融学、经济学、公司财务、公司金融、投资分析、证券分析、市场研究、行情解读、行情分析、金融科技新闻咨询解读等金融领域专业知识。你的任务是站在用户视角,评估金融研究Agent生成的研究报告的**数据支撑**。请仔细阅读以下信息,并按照给定的评分标准进行评估。

# 待评估信息
1. 这是用户的问题:<user_question>{query}</user_question>
2. 这是金融研究Agent生成的研究报告:<analyst_answer>{answer}</analyst_answer>
3. 这是报告生成的日期:<date>{chat_date}</date>

# 评估标准

## **底线问题标准(直接得1分)**
如果报告满足以下任意一条,直接得1分:
1. 报告内容违反法律、常识和基本金融知识,属于致命错误!!!
2. **严格要求报告的核心分析对象与问题主体一致**:若报告的核心分析对象和<user_question>询问的主体不一致,属于致命错误!!!
3. **严格要求报告内容前后一致**:若报告的关键点、关键指标、核心结论、观点、逻辑等内容存在前后矛盾和不一致情况,会给用户带来困惑,属于致命错误!!!
4. **数据与事实真实性**: 报告中引用的关键数据、事实、事件或引文必须是真实可查的。若出现明显的事实性错误或数据捏造（Hallucination）,属于致命错误!!!

## 数据支撑评分 (data_score)
评估结论和判断是否有充分的数据、事实或案例支撑,关键数据是否注明来源:

- **5分**: 数据支撑充分,所有关键结论都有详实数据支持,来源标注完整
- **4分**: 数据支撑良好,主要结论有数据支持,来源基本标注
- **3分**: 数据支撑一般,部分结论有数据支持,但存在主观推断
- **2分**: 数据支撑不足,大量主观推断,缺乏可靠数据支撑
- **1分**: 数据支撑缺失,纯主观判断或触犯底线问题标准（数据捏造）

# 评分流程
1. 理解<user_question>,并将其归类到一个或多个意图场景类别,以便后续评估参考。场景类别包括但不限于:行业研究、事件解读、个股分析、宏观分析、股票检索、其他等
2. 分析金融研究Agent生成的报告<analyst_answer>中的结论和判断,识别哪些有数据支撑,哪些缺乏支撑
3. **首先判断是否触犯底线问题标准**,如果触犯任意一条,直接得1分
4. 如果未触犯底线问题标准,根据数据支撑评分标准进行评分(1-5分)

# 输出要求

请以JSON格式输出评估结果（分数为1-5的整数）：
{{
    "score": <int (1-5)>,
    "reason": "<数据支撑评估理由，需要说明是否触犯底线问题标准，以及具体的数据支撑分析，包括哪些结论有充分数据支撑，哪些缺乏支撑>"
}}

JSON:
"""

# English Prompt
FINANCIAL_DATA_SUPPORT_PROMPT_EN = """# Task
You are a top-tier financial research report evaluation expert, proficient in finance, economics, corporate finance, investment analysis, securities analysis, market research, market interpretation, and financial technology news analysis. Your task is to evaluate the **data support** of research reports generated by financial research agents from the user's perspective. Please carefully read the following information and evaluate according to the given scoring criteria.

# Information to Evaluate
1. User's question: <user_question>{query}</user_question>
2. Research report generated by financial research agent: <analyst_answer>{answer}</analyst_answer>
3. Report generation date: <date>{chat_date}</date>

# Evaluation Criteria

## **Bottom-line Criteria (Direct score of 1)**
If the report meets any of the following conditions, it gets a direct score of 1:
1. Report content violates laws, common sense, and basic financial knowledge - a fatal error!!!
2. **Strict requirement for consistency between report's core analysis object and question subject**: If the report's core analysis object is inconsistent with the subject in <user_question>, it's a fatal error!!!
3. **Strict requirement for internal consistency**: If there are contradictions in key points, key indicators, core conclusions, viewpoints, or logic, causing user confusion, it's a fatal error!!!
4. **Data and factual authenticity**: Key data, facts, events, or citations in the report must be verifiable. Obvious factual errors or data fabrication (Hallucination) is a fatal error!!!

## Data Support Score (data_score)
Evaluate whether conclusions and judgments have sufficient data, facts, or case support, whether key data sources are cited:

- **5**: Sufficient data support, all key conclusions backed by solid data, complete source citations
- **4**: Good data support, main conclusions supported by data, sources basically cited
- **3**: Average data support, some conclusions supported by data, but subjective inferences exist
- **2**: Insufficient data support, extensive subjective inferences, lack of reliable data backing
- **1**: Missing data support, purely subjective judgments or violates bottom-line criteria (data fabrication)

# Evaluation Process
1. Understand <user_question> and categorize it into one or more intent scenario categories for subsequent evaluation reference. Categories include but are not limited to: industry research, event interpretation, individual stock analysis, macroeconomic analysis, stock screening, others, etc.
2. Analyze the conclusions and judgments in the report <analyst_answer> generated by financial research agent, identify which have data support and which lack support
3. **First determine if any bottom-line criteria is violated**, if any is violated, directly score 1
4. If no bottom-line criteria is violated, score according to data support scoring criteria (1-5)

# Output Requirement

Please output your evaluation in JSON format (score is an integer from 1-5):
{{
    "score": <int (1-5)>,
    "reason": "<Data support evaluation reasoning, need to explain whether bottom-line criteria is violated and specific data support analysis, including which conclusions have sufficient data support and which lack support>"
}}

JSON:
"""

# Build default template from prompts
DEFAULT_FINANCIAL_DATA_SUPPORT_TEMPLATE = PromptTemplate(
    messages={
        LanguageEnum.EN: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_DATA_SUPPORT_PROMPT_EN),
            ),
        ],
        LanguageEnum.ZH: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_DATA_SUPPORT_PROMPT_ZH),
            ),
        ],
    },
)


# Pydantic models for structured LLM output
class FinancialDataSupportEvaluationOutput(BaseModel):
    """Structured output model for financial data support evaluation LLM response."""

    score: int = Field(ge=1, le=5, description="数据支撑得分 (1-5)")
    reason: str = Field(default="", description="数据支撑判断原因")


def _normalize_score(score: Union[int, float]) -> float:
    """
    Normalize a 1-5 integer score to 0-1 continuous scale.

    Mapping:
    - 1 -> 0.0
    - 2 -> 0.25
    - 3 -> 0.5
    - 4 -> 0.75
    - 5 -> 1.0

    Args:
        score: Integer score from 1 to 5

    Returns:
        float: Normalized score from 0.0 to 1.0
    """
    # Clamp score to valid range [1, 5]
    score = max(1, min(5, float(score)))
    # Normalize: (score - 1) / 4
    return (score - 1) / 4.0


class FinancialReportDataSupportGrader(LLMGrader):
    """
    Financial deep research report data support evaluation grader.

    Evaluates whether a financial research agent's report conclusions and judgments
    have sufficient data, facts, or case support with proper source citations.

    This grader first checks for bottom-line criteria violations, then evaluates
    data support on a 1-5 scale, which is normalized to 0-1 for the final score.

    Attributes:
        name: Grader name
        model: ChatModelBase instance for evaluation
        language: Language for evaluation prompts

    Example:
        >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
        >>> api = OpenAIChatModel(api_key="...", model="gpt-4o")
        >>> grader = FinancialReportDataSupportGrader(model=api)
        >>> result = await grader.aevaluate(
        ...     messages=[
        ...         {"role": "user", "content": "分析某公司财报"},
        ...         {"role": "assistant", "content": "详细的财务分析报告..."}
        ...     ],
        ...     chat_date="2024-01-01"
        ... )
        >>> print(f"Score: {result.score}")  # normalized 0-1 score
    """

    def __init__(
        self,
        model: Union[BaseChatModel, dict],
        template: Optional[PromptTemplate] = DEFAULT_FINANCIAL_DATA_SUPPORT_TEMPLATE,
        language: LanguageEnum = LanguageEnum.ZH,
    ):
        """
        Initialize the FinancialDataSupportGrader.

        Args:
            model (Union[BaseChatModel, dict]): The chat model to use for evaluation.
                Can be either a BaseChatModel instance or a dictionary configuration.
            template (Optional[PromptTemplate]): The prompt template for data support evaluation.
                Defaults to DEFAULT_FINANCIAL_DATA_SUPPORT_TEMPLATE.
            language (LanguageEnum): Language for the evaluation prompt.
                Defaults to LanguageEnum.ZH (Chinese).

        Example:
            >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
            >>> model = OpenAIChatModel(api_key="...", model="gpt-4o")
            >>> grader = FinancialDataSupportGrader(model=model)
        """
        super().__init__(
            name="financial_report_data_support",
            mode=GraderMode.POINTWISE,
            description="Financial deep research report data support evaluation",
            model=model,
            template=template,
            language=language,
            structured_model=FinancialDataSupportEvaluationOutput,
        )

    def _extract_query_and_answer_from_messages(
        self,
        messages: List[Dict[str, Any]],
    ) -> tuple[str, str]:
        """
        Extract user query and answer from messages.

        Args:
            messages: List of message dicts (standard format).

        Returns:
            Tuple of (query, answer)
        """
        # Filter out system messages and unwrap nested structure
        messages = [msg.get("message", msg) for msg in messages]
        non_system_messages = [msg for msg in messages if msg.get("role", "") != "system"]

        if not non_system_messages:
            return "", ""

        # Extract user query (first non-system user message)
        query = ""
        if non_system_messages[0].get("role", "") == "user":
            query = non_system_messages[0].get("content", "")

        # Extract answer (last assistant message content)
        answer = ""
        for msg in reversed(non_system_messages):
            if msg.get("role", "") == "assistant" and msg.get("content", ""):
                answer = msg.get("content", "")
                break

        return query, answer

    async def aevaluate(
        self,
        messages: List[Dict[str, Any]],
        chat_date: Optional[str] = None,
    ) -> GraderScore:
        """
        Evaluate financial research report data support.

        The evaluation first checks for bottom-line criteria violations, then scores
        data support on a 1-5 scale, which is normalized to 0-1 for the final score.

        Args:
            messages: List of messages (standard format, including system, user, assistant)
                The "message" key for message can be optional.

            chat_date (Optional[str]): Date of report generation. If not provided,
                uses current system date (YYYY-MM-DD format)

        Returns:
            GraderScore: Data support evaluation score for the report (normalized 0.0-1.0)
                - score: Normalized data support score (0.0-1.0)
                - reason: Data support evaluation reasoning
                - metadata: Contains raw_score (1-5)

        Example:
            >>> result = await grader.aevaluate(
            ...     messages=[
            ...         {"role": "user", "content": "分析某公司的财务状况"},
            ...         {"role": "assistant", "content": "详细的财务分析报告..."}
            ...     ],
            ...     chat_date="2024-01-01"
            ... )
            >>> print(f"Data Support Score: {result.score}")
        """
        # Extract query and answer from messages
        query, answer = self._extract_query_and_answer_from_messages(messages)

        # Use provided chat_date if available, otherwise use current system date
        if chat_date is None:
            chat_date = datetime.now().strftime("%Y-%m-%d")

        if not query or not answer:
            logger.warning("Empty query or answer, returning error")
            return GraderError(name=self.name, error="Empty query or answer")

        try:
            # Call parent evaluation with formatted parameters
            result = await super().aevaluate(
                query=query,
                answer=answer,
                chat_date=chat_date,
            )

            # Normalize score from 1-5 to 0-1
            normalized_score = _normalize_score(result.score)

            return GraderScore(
                name=self.name,
                score=normalized_score,
                reason=result.reason,
                metadata=result.metadata,
            )

        except Exception as e:
            logger.error(f"Error evaluating {self.name}: {e}")
            return GraderError(name=self.name, error=str(e))
