"""
Financial Deep Research Report Resolution Grader for Research Report Evaluation

This module provides a grader system for evaluating whether financial research agent
reports adequately resolve user research questions based on comprehensive evaluation criteria.
"""

import textwrap
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Union

from loguru import logger
from pydantic import BaseModel, Field

from rm_gallery.core.graders.base_grader import GraderMode, GraderScore
from rm_gallery.core.graders.llm_grader import LLMGrader
from rm_gallery.core.graders.schema import GraderError
from rm_gallery.core.models.base_chat_model import BaseChatModel
from rm_gallery.core.models.schema.oai.response import ChatResponse
from rm_gallery.core.models.schema.oai.message import ChatMessage
from rm_gallery.core.models.schema.prompt_template import LanguageEnum, PromptTemplate

# pylint: disable=line-too-long

# Chinese Prompt
FINANCIAL_REPORT_RESOLUTION_PROMPT_ZH = """# 任务
你是一个顶级的金融研究报告评估专家,精通金融学、经济学、公司财务、公司金融、投资分析、证券分析、市场研究、行情解读、行情分析、金融科技新闻咨询解读等金融领域专业知识。你的任务是站在用户视角,评估金融研究Agent生成的研究报告的质量。请仔细阅读以下信息,并按照给定的评分标准进行评估。

# 待评估信息
1. 这是用户的问题:<user_question>{query}</user_question>
2. 这是金融研究Agent生成的研究报告:<analyst_answer>{answer}</analyst_answer>
3. 这是报告生成的日期:<date>{chat_date}</date>

# 评估维度与标准

## **底线问题标准(直接得1分)**
如果报告满足以下任意一条,所有维度直接得1分:
1. 报告内容违反法律、常识和基本金融知识,属于致命错误!!!
2. **严格要求报告的核心分析对象与问题主体一致**:若报告的核心分析对象和<user_question>询问的主体不一致,属于致命错误!!!
3. **严格要求报告内容前后一致**:若报告的关键点、关键指标、核心结论、观点、逻辑等内容存在前后矛盾和不一致情况,会给用户带来困惑,属于致命错误!!!
4. **数据与事实真实性**: 报告中引用的关键数据、事实、事件或引文必须是真实可查的。若出现明显的事实性错误或数据捏造（Hallucination）,属于致命错误!!!

## 1. 需求满足度与深度维度

### 1.1 精准性评分 (precision_score)
评估报告是否直接针对用户问题的核心诉求,报告的实体对象、内容与用户问题精确匹配程度:

- **5分**: 完全精准,报告核心内容与用户问题完美匹配,实体对象、分析重点完全一致,直击问题核心
- **4分**: 高度精准,报告主要内容准确针对用户问题,可能有少量扩展但不影响核心匹配度
- **3分**: 基本精准,报告大致针对用户问题,但存在一定程度的偏离或泛化
- **2分**: 精准度不足,报告部分偏离用户问题,答非所问的情况较明显
- **1分**: 完全不精准,严重答非所问或触犯底线问题标准

**注意**: 由于金融合规性约束,如果<user_question>要求预测具体资产的未来收益或涨跌,可以不提供明确预测结论,转而提供分析框架和风险提示,这种情况不应扣分。

### 1.2 完整性评分 (completeness_score)
评估报告是否完整覆盖用户问题中的所有关键点和需求维度（如标的、时间范围、对比对象、核心指标等）:

- **5分**: 完全完整,全面覆盖所有关键点和需求维度,无遗漏
- **4分**: 高度完整,覆盖绝大部分关键点,可能有极少量非核心维度未涉及
- **3分**: 基本完整,覆盖主要关键点,但有部分重要维度缺失或分析不足
- **2分**: 完整性不足,多个关键点缺失,信息覆盖有明显缺口
- **1分**: 严重不完整,大量关键点缺失或触犯底线问题标准

**注意**: 若关键信息不足,应在报告中明确指出信息缺口并说明所做的合理假设,而不是直接编造结论。

### 1.3 相关性评分 (relevance_score)
评估报告内容与用户问题的相关性,是否避免引入无关内容:

- **5分**: 高度相关,所有内容紧密围绕用户问题,高度聚焦
- **4分**: 较为相关,主要内容相关,可能有少量合理扩展
- **3分**: 基本相关,大部分内容相关但存在一定无关信息
- **2分**: 相关性不足,包含较多无关或偏离主题的内容
- **1分**: 严重不相关,内容严重偏离问题或触犯底线问题标准

### 1.4 时效性评分 (timeliness_score)
评估报告是否基于最新的市场信息、数据和分析论据:

- **5分**: 时效性极佳,使用最新数据和信息,具有很强的时效价值
- **4分**: 时效性良好,数据较新,符合当前市场情况
- **3分**: 时效性一般,数据基本及时但可能略有滞后
- **2分**: 时效性不足,数据明显滞后,不能反映当前市场状况
- **1分**: 时效性极差,使用过时数据或触犯底线问题标准

## 2. 专业性和实用性维度

### 2.1 逻辑严谨性评分 (logic_score)
评估分析是否符合金融原理和经济学逻辑,因果关系是否清晰:

- **5分**: 逻辑完全严谨,分析框架清晰,因果关系明确,推理过程无懈可击
- **4分**: 逻辑基本严谨,分析合理,可能有极少量推理跳跃但不影响整体
- **3分**: 逻辑尚可,存在一定逻辑跳跃或未说明的假设,但不影响主要结论
- **2分**: 逻辑性不足,存在明显逻辑漏洞或因果关系不清
- **1分**: 逻辑混乱,存在严重逻辑错误或触犯底线问题标准

### 2.2 数据支撑评分 (data_score)
评估结论和判断是否有充分的数据、事实或案例支撑,关键数据是否注明来源:

- **5分**: 数据支撑充分,所有关键结论都有详实数据支持,来源标注完整
- **4分**: 数据支撑良好,主要结论有数据支持,来源基本标注
- **3分**: 数据支撑一般,部分结论有数据支持,但存在主观推断
- **2分**: 数据支撑不足,大量主观推断,缺乏可靠数据支撑
- **1分**: 数据支撑缺失,纯主观判断或触犯底线问题标准（数据捏造）

### 2.3 报告结构与可读性评分 (structure_score)
评估报告结构是否清晰、层次是否分明、语言是否专业且易于理解:

- **5分**: 结构完美,层次清晰分明（含摘要、正文、结论、风险提示等）,语言专业易懂,善用列表等形式
- **4分**: 结构良好,层次较为清晰,语言专业,可读性强
- **3分**: 结构基本合理,层次尚可,语言尚可,基本满足可读性要求
- **2分**: 结构混乱,层次不清,语言表达欠佳,可读性差
- **1分**: 结构严重混乱,难以阅读或触犯底线问题标准

# 评分流程
1. 理解<user_question>,并将其归类到一个或多个意图场景类别,以便后续评估参考。场景类别包括但不限于:行业研究、事件解读、个股分析、宏观分析、股票检索、其他等
2. 梳理<user_question>和金融研究Agent生成的报告<analyst_answer>中的关键点
3. 根据**评估维度与标准**,对每个维度进行评分(1-5分)。注意:如果命中任意一条<底线问题标准>,相关维度直接得1分

# 输出要求

请以JSON格式输出评估结果（每个维度的分数为1-5的整数）：
{{
    "precision_score": <int (1-5)>,
    "precision_reason": "<精准性评估理由>",
    "completeness_score": <int (1-5)>,
    "completeness_reason": "<完整性评估理由>",
    "relevance_score": <int (1-5)>,
    "relevance_reason": "<相关性评估理由>",
    "timeliness_score": <int (1-5)>,
    "timeliness_reason": "<时效性评估理由>",
    "logic_score": <int (1-5)>,
    "logic_reason": "<逻辑严谨性评估理由>",
    "data_score": <int (1-5)>,
    "data_reason": "<数据支撑评估理由>",
    "structure_score": <int (1-5)>,
    "structure_reason": "<报告结构与可读性评估理由>"
}}

JSON:
"""

# English Prompt
FINANCIAL_REPORT_RESOLUTION_PROMPT_EN = """# Task
You are a top-tier financial research report evaluation expert, proficient in finance, economics, corporate finance, investment analysis, securities analysis, market research, market interpretation, and financial technology news analysis. Your task is to evaluate the quality of research reports generated by financial research agents from the user's perspective. Please carefully read the following information and evaluate according to the given scoring criteria.

# Information to Evaluate
1. User's question: <user_question>{query}</user_question>
2. Research report generated by financial research agent: <analyst_answer>{answer}</analyst_answer>
3. Report generation date: <date>{chat_date}</date>

# Evaluation Dimensions and Criteria

## **Bottom-line Criteria (Direct score of 1)**
If the report meets any of the following conditions, the relevant dimension gets a direct score of 1:
1. Report content violates laws, common sense, and basic financial knowledge - a fatal error!!!
2. **Strict requirement for consistency between report's core analysis object and question subject**: If the report's core analysis object is inconsistent with the subject in <user_question>, it's a fatal error!!!
3. **Strict requirement for internal consistency**: If there are contradictions in key points, key indicators, core conclusions, viewpoints, or logic, causing user confusion, it's a fatal error!!!
4. **Data and factual authenticity**: Key data, facts, events, or citations in the report must be verifiable. Obvious factual errors or data fabrication (Hallucination) is a fatal error!!!

## 1. Requirement Satisfaction and Depth Dimensions

### 1.1 Precision Score (precision_score)
Evaluate whether the report directly addresses the core request of the user's question, and the degree of exact match between report entities, content and user question:

- **5**: Completely precise, report core content perfectly matches user question, entities and analysis focus fully aligned, directly addresses the core
- **4**: Highly precise, report main content accurately targets user question, may have minor extensions but doesn't affect core alignment
- **3**: Basically precise, report roughly addresses user question but with some degree of deviation or generalization
- **2**: Insufficient precision, report partially deviates from user question, obvious cases of missing the point
- **1**: Completely imprecise, seriously missing the point or violates bottom-line criteria

**Note**: Due to financial compliance constraints, if <user_question> requires predictions of specific asset returns or price movements, providing analytical frameworks and risk warnings instead of explicit predictions is acceptable and should not be penalized.

### 1.2 Completeness Score (completeness_score)
Evaluate whether the report fully covers all key points and requirement dimensions in the user question (such as targets, time ranges, comparison objects, core indicators, etc.):

- **5**: Completely complete, comprehensively covers all key points and requirement dimensions, no omissions
- **4**: Highly complete, covers vast majority of key points, may have minimal non-core dimensions not covered
- **3**: Basically complete, covers main key points but some important dimensions are missing or insufficiently analyzed
- **2**: Insufficient completeness, multiple key points missing, obvious gaps in information coverage
- **1**: Seriously incomplete, numerous key points missing or violates bottom-line criteria

**Note**: If key information is insufficient, the report should explicitly point out information gaps and explain reasonable assumptions made, rather than fabricating conclusions.

### 1.3 Relevance Score (relevance_score)
Evaluate the relevance of report content to the user question, whether irrelevant content is avoided:

- **5**: Highly relevant, all content closely围绕 user question, highly focused
- **4**: Fairly relevant, main content relevant, may have minor reasonable extensions
- **3**: Basically relevant, most content relevant but some irrelevant information exists
- **2**: Insufficient relevance, contains considerable irrelevant or off-topic content
- **1**: Seriously irrelevant, content seriously deviates from question or violates bottom-line criteria

### 1.4 Timeliness Score (timeliness_score)
Evaluate whether the report is based on the latest market information, data, and analytical evidence:

- **5**: Excellent timeliness, uses latest data and information, strong temporal value
- **4**: Good timeliness, relatively new data, fits current market conditions
- **3**: Average timeliness, data basically timely but may be slightly outdated
- **2**: Insufficient timeliness, obviously outdated data, doesn't reflect current market conditions
- **1**: Extremely poor timeliness, uses outdated data or violates bottom-line criteria

## 2. Professionalism and Practicality Dimensions

### 2.1 Logical Rigor Score (logic_score)
Evaluate whether analysis follows financial principles and economic logic, whether causal relationships are clear:

- **5**: Completely rigorous logic, clear analytical framework, explicit causal relationships, flawless reasoning
- **4**: Basically rigorous logic, reasonable analysis, may have minimal reasoning jumps but doesn't affect overall
- **3**: Acceptable logic, some logical jumps or unexplained assumptions exist, but doesn't affect main conclusions
- **2**: Insufficient logic, obvious logical flaws or unclear causal relationships
- **1**: Chaotic logic, serious logical errors or violates bottom-line criteria

### 2.2 Data Support Score (data_score)
Evaluate whether conclusions and judgments have sufficient data, facts, or case support, whether key data sources are cited:

- **5**: Sufficient data support, all key conclusions backed by solid data, complete source citations
- **4**: Good data support, main conclusions supported by data, sources basically cited
- **3**: Average data support, some conclusions supported by data, but subjective inferences exist
- **2**: Insufficient data support, extensive subjective inferences, lack of reliable data backing
- **1**: Missing data support, purely subjective judgments or violates bottom-line criteria (data fabrication)

### 2.3 Report Structure and Readability Score (structure_score)
Evaluate whether report structure is clear, hierarchy well-defined, language professional and easy to understand:

- **5**: Perfect structure, clear hierarchy (including summary, body, conclusion, risk warnings, etc.), professional and accessible language, good use of lists
- **4**: Good structure, relatively clear hierarchy, professional language, strong readability
- **3**: Basically reasonable structure, acceptable hierarchy, acceptable language, basically meets readability requirements
- **2**: Chaotic structure, unclear hierarchy, poor language expression, poor readability
- **1**: Seriously chaotic structure, difficult to read or violates bottom-line criteria

# Evaluation Process
1. Understand <user_question> and categorize it into one or more intent scenario categories for subsequent evaluation reference. Categories include but are not limited to: industry research, event interpretation, individual stock analysis, macroeconomic analysis, stock screening, others, etc.
2. Sort out key points in <user_question> and the report <analyst_answer> generated by financial research agent
3. Score each dimension (1-5) according to **Evaluation Dimensions and Criteria**. Note: If any <Bottom-line Criteria> is met, the relevant dimension gets a direct score of 1

# Output Requirement

Please output your evaluation in JSON format (each dimension score is an integer from 1-5):
{{
    "precision_score": <int (1-5)>,
    "precision_reason": "<Precision evaluation reasoning>",
    "completeness_score": <int (1-5)>,
    "completeness_reason": "<Completeness evaluation reasoning>",
    "relevance_score": <int (1-5)>,
    "relevance_reason": "<Relevance evaluation reasoning>",
    "timeliness_score": <int (1-5)>,
    "timeliness_reason": "<Timeliness evaluation reasoning>",
    "logic_score": <int (1-5)>,
    "logic_reason": "<Logical rigor evaluation reasoning>",
    "data_score": <int (1-5)>,
    "data_reason": "<Data support evaluation reasoning>",
    "structure_score": <int (1-5)>,
    "structure_reason": "<Report structure and readability evaluation reasoning>"
}}

JSON:
"""

# Build default template from prompts
DEFAULT_FINANCIAL_RESEARCH_TEMPLATE = PromptTemplate(
    messages={
        LanguageEnum.EN: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_REPORT_RESOLUTION_PROMPT_EN),
            ),
        ],
        LanguageEnum.ZH: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_REPORT_RESOLUTION_PROMPT_ZH),
            ),
        ],
    },
)


# Pydantic models for structured LLM output
class DimensionEvaluation(BaseModel):
    """Single dimension evaluation from LLM."""

    dimension_name: str = Field(description="Dimension name")
    score: int = Field(ge=1, le=5, description="Score (1-5)")
    reason: str = Field(default="", description="Detailed evaluation reasoning for this dimension")


class FinancialResearchEvaluationOutput(BaseModel):
    """Structured output model for financial research evaluation LLM response."""

    precision_score: int = Field(ge=1, le=5, description="精准性得分 (1-5)")
    precision_reason: str = Field(default="", description="精准性判断原因")

    completeness_score: int = Field(ge=1, le=5, description="完整性得分 (1-5)")
    completeness_reason: str = Field(default="", description="完整性判断原因")

    relevance_score: int = Field(ge=1, le=5, description="相关性得分 (1-5)")
    relevance_reason: str = Field(default="", description="相关性判断原因")

    timeliness_score: int = Field(ge=1, le=5, description="时效性得分 (1-5)")
    timeliness_reason: str = Field(default="", description="时效性判断原因")

    logic_score: int = Field(ge=1, le=5, description="逻辑严谨性得分 (1-5)")
    logic_reason: str = Field(default="", description="逻辑严谨性判断原因")

    data_score: int = Field(ge=1, le=5, description="数据支撑得分 (1-5)")
    data_reason: str = Field(default="", description="数据支撑判断原因")

    structure_score: int = Field(ge=1, le=5, description="报告结构与可读性得分 (1-5)")
    structure_reason: str = Field(default="", description="报告结构与可读性判断原因")


def _normalize_score(score: Union[int, float]) -> float:
    """
    Normalize a 1-5 integer score to 0-1 continuous scale.

    Mapping:
    - 1 -> 0.0
    - 2 -> 0.25
    - 3 -> 0.5
    - 4 -> 0.75
    - 5 -> 1.0

    Args:
        score: Integer score from 1 to 5

    Returns:
        float: Normalized score from 0.0 to 1.0
    """
    # Clamp score to valid range [1, 5]
    score = max(1, min(5, float(score)))
    # Normalize: (score - 1) / 4
    return (score - 1) / 4.0


class FinancialReportResolutionGrader(LLMGrader):
    """
    Financial deep research report resolution rate evaluation grader.

    Evaluates whether a financial research agent's report adequately resolves
    the user's research questions based on comprehensive evaluation criteria
    including requirement satisfaction, professionalism, and practical applicability.

    This grader uses an LLM as judge approach with a detailed scoring rubric
    that covers multiple dimensions of research report quality. Each dimension
    uses a 1-5 scoring system, which is then weighted and normalized to 0-1 scale.

    Dimension weights (based on original 100-point system):
    - precision: 25% (25/100)
    - completeness: 20% (20/100)
    - relevance: 10% (10/100)
    - timeliness: 10% (10/100)
    - logic: 10% (10/100)
    - data: 15% (15/100)
    - structure: 10% (10/100)

    Attributes:
        name: Grader name
        model: ChatModelBase instance for evaluation
        language: Language for evaluation prompts
        resolution_threshold: Threshold for determining if the report is resolved
            (default: 0.9, on normalized 0-1 scale)

    Example:
        >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
        >>> api = OpenAIChatModel(api_key="...", model="gpt-4o")
        >>> grader = FinancialReportResolutionGrader(model=api, resolution_threshold=0.85)
        >>> result = await grader.aevaluate(
        ...     messages=[
        ...         {"role": "user", "content": "分析某公司财报"},
        ...         {"role": "assistant", "content": "详细的财务分析报告..."}
        ...     ],
        ...     chat_date="2024-01-01"
        ... )
        >>> print(f"Score: {result.score}")  # weighted and normalized score
    """

    # Dimension weights based on original 100-point system
    DIMENSION_WEIGHTS = {
        "precision": 0.25,  # 25/100
        "completeness": 0.20,  # 20/100
        "relevance": 0.10,  # 10/100
        "timeliness": 0.10,  # 10/100
        "logic": 0.10,  # 10/100
        "data": 0.15,  # 15/100
        "structure": 0.10,  # 10/100
    }

    @staticmethod
    def _create_financial_research_callback(
        language: LanguageEnum = LanguageEnum.ZH,
    ) -> Callable[[Any], Dict[str, Any]]:
        """
        Create a callback function to process dimension evaluations into final score and reason.

        This callback:
        1. Extracts dimension scores and reasons from ChatResponse.parsed
        2. Normalizes each dimension score from 1-5 to 0-1 scale
        3. Applies dimension weights based on original 100-point system
        4. Calculates weighted average as final score
        5. Concatenates dimension reasons into final reason string

        Args:
            language: Language for generating the aggregated reason

        Returns:
            Callable that processes ChatResponse into metadata dict with score and reason
        """

        def callback(response: ChatResponse) -> Dict[str, Any]:
            # Extract metadata from ChatResponse
            parsed = response.parsed or {}

            # Extract dimension scores and reasons from parsed
            try:
                precision_score = parsed.get("precision_score", 1)
                precision_reason = parsed.get("precision_reason", "")

                completeness_score = parsed.get("completeness_score", 1)
                completeness_reason = parsed.get("completeness_reason", "")

                relevance_score = parsed.get("relevance_score", 1)
                relevance_reason = parsed.get("relevance_reason", "")

                timeliness_score = parsed.get("timeliness_score", 1)
                timeliness_reason = parsed.get("timeliness_reason", "")

                logic_score = parsed.get("logic_score", 1)
                logic_reason = parsed.get("logic_reason", "")

                data_score = parsed.get("data_score", 1)
                data_reason = parsed.get("data_reason", "")

                structure_score = parsed.get("structure_score", 1)
                structure_reason = parsed.get("structure_reason", "")

            except Exception as e:
                logger.warning(f"Failed to extract dimension scores: {e}")
                return {
                    "score": 0.0,
                    "reason": (
                        "Failed to extract evaluation scores." if language == LanguageEnum.EN else "无法提取评估分数。"
                    ),
                    "dimension_scores": {},
                }

            # Normalize each dimension score from 1-5 to 0-1
            normalized_scores = {
                "precision": _normalize_score(precision_score),
                "completeness": _normalize_score(completeness_score),
                "relevance": _normalize_score(relevance_score),
                "timeliness": _normalize_score(timeliness_score),
                "logic": _normalize_score(logic_score),
                "data": _normalize_score(data_score),
                "structure": _normalize_score(structure_score),
            }

            # Calculate weighted average score
            weighted_score = sum(
                score * FinancialReportResolutionGrader.DIMENSION_WEIGHTS[dim]
                for dim, score in normalized_scores.items()
            )

            # Build aggregated reason string
            if language == LanguageEnum.ZH:
                reason_parts = [
                    f"【精准性】{precision_score}/5分: {precision_reason}",
                    f"【完整性】{completeness_score}/5分: {completeness_reason}",
                    f"【相关性】{relevance_score}/5分: {relevance_reason}",
                    f"【时效性】{timeliness_score}/5分: {timeliness_reason}",
                    f"【逻辑严谨性】{logic_score}/5分: {logic_reason}",
                    f"【数据支撑】{data_score}/5分: {data_reason}",
                    f"【报告结构与可读性】{structure_score}/5分: {structure_reason}",
                ]
            else:
                reason_parts = [
                    f"[Precision] {precision_score}/5: {precision_reason}",
                    f"[Completeness] {completeness_score}/5: {completeness_reason}",
                    f"[Relevance] {relevance_score}/5: {relevance_reason}",
                    f"[Timeliness] {timeliness_score}/5: {timeliness_reason}",
                    f"[Logical Rigor] {logic_score}/5: {logic_reason}",
                    f"[Data Support] {data_score}/5: {data_reason}",
                    f"[Report Structure] {structure_score}/5: {structure_reason}",
                ]

            reason = "\n".join(reason_parts)

            return {
                "score": weighted_score,
                "reason": reason,
                "dimension_scores": {
                    "precision": {"raw": precision_score, "normalized": normalized_scores["precision"]},
                    "completeness": {"raw": completeness_score, "normalized": normalized_scores["completeness"]},
                    "relevance": {"raw": relevance_score, "normalized": normalized_scores["relevance"]},
                    "timeliness": {"raw": timeliness_score, "normalized": normalized_scores["timeliness"]},
                    "logic": {"raw": logic_score, "normalized": normalized_scores["logic"]},
                    "data": {"raw": data_score, "normalized": normalized_scores["data"]},
                    "structure": {"raw": structure_score, "normalized": normalized_scores["structure"]},
                },
            }

        return callback

    def __init__(
        self,
        model: Union[BaseChatModel, dict],
        template: Optional[PromptTemplate] = DEFAULT_FINANCIAL_RESEARCH_TEMPLATE,
        language: LanguageEnum = LanguageEnum.ZH,
        resolution_threshold: float = 0.9,
    ):
        """
        Initialize the FinancialReportResolutionGrader.

        Args:
            model (Union[BaseChatModel, dict]): The chat model to use for evaluation.
                Can be either a BaseChatModel instance or a dictionary configuration.
            template (Optional[PromptTemplate]): The prompt template for financial research evaluation.
                Defaults to DEFAULT_FINANCIAL_RESEARCH_TEMPLATE.
            language (LanguageEnum): Language for the evaluation prompt.
                Defaults to LanguageEnum.ZH (Chinese).
            resolution_threshold (float): Threshold for determining if the report is resolved.
                Scores greater than or equal to this value are considered resolved.
                Defaults to 0.9 (90%). Note: This is on 0-1 normalized scale.

        Example:
            >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
            >>> model = OpenAIChatModel(api_key="...", model="gpt-4o")
            >>> grader = FinancialReportResolutionGrader(model=model, resolution_threshold=0.85)
        """
        super().__init__(
            name="financial_report_resolution",
            mode=GraderMode.POINTWISE,
            description="Financial deep research report resolution evaluation with multi-dimensional scoring",
            model=model,
            template=template,
            language=language,
            structured_model=FinancialResearchEvaluationOutput,
            callback=self._create_financial_research_callback(language=language),
        )
        self.resolution_threshold = resolution_threshold

    def _extract_query_and_answer_from_messages(
        self,
        messages: List[Dict[str, Any]],
    ) -> tuple[str, str]:
        """
        Extract user query and answer from messages.

        Args:
            messages: List of message dicts (standard format).

        Returns:
            Tuple of (query, answer)
        """
        # Filter out system messages and unwrap nested structure
        messages = [msg.get("message", msg) for msg in messages]
        non_system_messages = [msg for msg in messages if msg.get("role", "") != "system"]

        if not non_system_messages:
            return "", ""

        # Extract user query (first non-system user message)
        query = ""
        if non_system_messages[0].get("role", "") == "user":
            query = non_system_messages[0].get("content", "")

        # Extract answer (last assistant message content)
        answer = ""
        for msg in reversed(non_system_messages):
            if msg.get("role", "") == "assistant" and msg.get("content", ""):
                answer = msg.get("content", "")
                break

        return query, answer

    async def aevaluate(
        self,
        messages: List[Dict[str, Any]],
        chat_date: Optional[str] = None,
    ) -> GraderScore:
        """
        Evaluate financial research report quality from standard messages format.

        The evaluation uses 1-5 integer scores in LLM prompts for each dimension, then:
        1. Normalizes each dimension score to 0-1 scale (1->0.0, 2->0.25, 3->0.5, 4->0.75, 5->1.0)
        2. Applies dimension weights based on original 100-point system
        3. Calculates weighted average as final score
        4. Concatenates dimension reasons into final reason string

        Args:
            messages: List of messages (standard format, including system, user, assistant)
                The "message" key for message can be optional.

                Example without "message" wrapper:
                ```
                [
                  {"role": "system", "content": "You are a financial analyst..."},
                  {"role": "user", "content": "分析某公司的财务状况"},
                  {"role": "assistant", "content": "详细的财务分析报告..."}
                ]
                ```

                Example with "message" wrapper:
                ```
                [
                  {"message": {"role": "system", "content": "You are a financial analyst..."}},
                  {"message": {"role": "user", "content": "分析某公司的财务状况"}},
                  {"message": {"role": "assistant", "content": "详细的财务分析报告..."}}
                ]
                ```


            chat_date (Optional[str]): Date of report generation. If not provided,
                uses current system date (YYYY-MM-DD format)

        Returns:
            GraderScore: Comprehensive evaluation score for the report (normalized 0.0-1.0)
                - score: Weighted average score (normalized 0.0-1.0)
                - reason: Concatenated dimension evaluation reasons
                - metadata: Contains dimension_scores with raw and normalized values,
                           is_resolved flag, and resolution_threshold

        Example:
            >>> result = await grader.aevaluate(
            ...     messages=[
            ...         {"role": "user", "content": "分析某公司的财务状况"},
            ...         {"role": "assistant", "content": "详细的财务分析报告..."}
            ...     ],
            ...     chat_date="2024-01-01"
            ... )
            >>> print(f"Overall Score: {result.score}")
            >>> print(f"Is Resolved: {result.metadata['is_resolved']}")
        """
        # Extract query and answer from messages
        query, answer = self._extract_query_and_answer_from_messages(messages)

        # Use provided chat_date if available, otherwise use current system date
        if chat_date is None:
            # Use current system date in YYYY-MM-DD format
            chat_date = datetime.now().strftime("%Y-%m-%d")

        if not query or not answer:
            logger.warning("Empty query or answer, returning error")
            return GraderError(
                name=self.name,
                error="Empty query or answer"
            )

        try:
            # Call parent evaluation with formatted parameters
            # The callback handles dimension-level to final score/reason conversion
            result = await super().aevaluate(
                query=query,
                answer=answer,
                chat_date=chat_date,
            )

            # Determine resolution status using the specified threshold
            # Original standard: ≥90/100 = 0.9 on normalized scale
            is_resolved = result.score >= self.resolution_threshold

            # Add additional metadata
            metadata = result.metadata or {}
            metadata["is_resolved"] = is_resolved
            metadata["resolution_threshold"] = self.resolution_threshold
            metadata["evaluation_type"] = "financial_report_resolution"

            return GraderScore(
                name=self.name,
                score=result.score,
                reason=result.reason,
                metadata=metadata,
            )

        except Exception as e:
            logger.error(f"Error evaluating {self.name}: {e}")
            return GraderError(name=self.name, error=str(e))
