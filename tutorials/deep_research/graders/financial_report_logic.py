"""
Financial Deep Research Report Logical Rigor Grader

This module provides a grader for evaluating the logical rigor of financial research
agent reports - whether the analysis follows financial principles and has clear causal relationships.
"""

import textwrap
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from loguru import logger
from pydantic import BaseModel, Field

from rm_gallery.core.graders.base_grader import GraderMode, GraderScore
from rm_gallery.core.graders.llm_grader import LLMGrader
from rm_gallery.core.graders.schema import GraderError
from rm_gallery.core.models.base_chat_model import BaseChatModel
from rm_gallery.core.models.schema.oai.message import ChatMessage
from rm_gallery.core.models.schema.prompt_template import LanguageEnum, PromptTemplate

# pylint: disable=line-too-long

# Chinese Prompt
FINANCIAL_LOGIC_PROMPT_ZH = """# 任务
你是一个顶级的金融研究报告评估专家,精通金融学、经济学、公司财务、公司金融、投资分析、证券分析、市场研究、行情解读、行情分析、金融科技新闻咨询解读等金融领域专业知识。你的任务是站在用户视角,评估金融研究Agent生成的研究报告的**逻辑严谨性**。请仔细阅读以下信息,并按照给定的评分标准进行评估。

# 待评估信息
1. 这是用户的问题:<user_question>{query}</user_question>
2. 这是金融研究Agent生成的研究报告:<analyst_answer>{answer}</analyst_answer>
3. 这是报告生成的日期:<date>{chat_date}</date>

# 评估标准

## **底线问题标准(直接得1分)**
如果报告满足以下任意一条,直接得1分:
1. 报告内容违反法律、常识和基本金融知识,属于致命错误!!!
2. **严格要求报告的核心分析对象与问题主体一致**:若报告的核心分析对象和<user_question>询问的主体不一致,属于致命错误!!!
3. **严格要求报告内容前后一致**:若报告的关键点、关键指标、核心结论、观点、逻辑等内容存在前后矛盾和不一致情况,会给用户带来困惑,属于致命错误!!!
4. **数据与事实真实性**: 报告中引用的关键数据、事实、事件或引文必须是真实可查的。若出现明显的事实性错误或数据捏造（Hallucination）,属于致命错误!!!

## 逻辑严谨性评分 (logic_score)
评估分析是否符合金融原理和经济学逻辑,因果关系是否清晰:

- **5分**: 逻辑完全严谨,分析框架清晰,因果关系明确,推理过程无懈可击
- **4分**: 逻辑基本严谨,分析合理,可能有极少量推理跳跃但不影响整体
- **3分**: 逻辑尚可,存在一定逻辑跳跃或未说明的假设,但不影响主要结论
- **2分**: 逻辑性不足,存在明显逻辑漏洞或因果关系不清
- **1分**: 逻辑混乱,存在严重逻辑错误或触犯底线问题标准

# 评分流程
1. 理解<user_question>,并将其归类到一个或多个意图场景类别,以便后续评估参考。场景类别包括但不限于:行业研究、事件解读、个股分析、宏观分析、股票检索、其他等
2. 分析金融研究Agent生成的报告<analyst_answer>中的逻辑框架、推理过程、因果关系
3. **首先判断是否触犯底线问题标准**,如果触犯任意一条,直接得1分
4. 如果未触犯底线问题标准,根据逻辑严谨性评分标准进行评分(1-5分)

# 输出要求

请以JSON格式输出评估结果（分数为1-5的整数）：
{{
    "score": <int (1-5)>,
    "reason": "<逻辑严谨性评估理由，需要说明是否触犯底线问题标准，以及具体的逻辑分析，包括分析框架、推理过程、因果关系的评估>"
}}

JSON:
"""

# English Prompt
FINANCIAL_LOGIC_PROMPT_EN = """# Task
You are a top-tier financial research report evaluation expert, proficient in finance, economics, corporate finance, investment analysis, securities analysis, market research, market interpretation, and financial technology news analysis. Your task is to evaluate the **logical rigor** of research reports generated by financial research agents from the user's perspective. Please carefully read the following information and evaluate according to the given scoring criteria.

# Information to Evaluate
1. User's question: <user_question>{query}</user_question>
2. Research report generated by financial research agent: <analyst_answer>{answer}</analyst_answer>
3. Report generation date: <date>{chat_date}</date>

# Evaluation Criteria

## **Bottom-line Criteria (Direct score of 1)**
If the report meets any of the following conditions, it gets a direct score of 1:
1. Report content violates laws, common sense, and basic financial knowledge - a fatal error!!!
2. **Strict requirement for consistency between report's core analysis object and question subject**: If the report's core analysis object is inconsistent with the subject in <user_question>, it's a fatal error!!!
3. **Strict requirement for internal consistency**: If there are contradictions in key points, key indicators, core conclusions, viewpoints, or logic, causing user confusion, it's a fatal error!!!
4. **Data and factual authenticity**: Key data, facts, events, or citations in the report must be verifiable. Obvious factual errors or data fabrication (Hallucination) is a fatal error!!!

## Logical Rigor Score (logic_score)
Evaluate whether analysis follows financial principles and economic logic, whether causal relationships are clear:

- **5**: Completely rigorous logic, clear analytical framework, explicit causal relationships, flawless reasoning
- **4**: Basically rigorous logic, reasonable analysis, may have minimal reasoning jumps but doesn't affect overall
- **3**: Acceptable logic, some logical jumps or unexplained assumptions exist, but doesn't affect main conclusions
- **2**: Insufficient logic, obvious logical flaws or unclear causal relationships
- **1**: Chaotic logic, serious logical errors or violates bottom-line criteria

# Evaluation Process
1. Understand <user_question> and categorize it into one or more intent scenario categories for subsequent evaluation reference. Categories include but are not limited to: industry research, event interpretation, individual stock analysis, macroeconomic analysis, stock screening, others, etc.
2. Analyze the logical framework, reasoning process, and causal relationships in the report <analyst_answer> generated by financial research agent
3. **First determine if any bottom-line criteria is violated**, if any is violated, directly score 1
4. If no bottom-line criteria is violated, score according to logical rigor scoring criteria (1-5)

# Output Requirement

Please output your evaluation in JSON format (score is an integer from 1-5):
{{
    "score": <int (1-5)>,
    "reason": "<Logical rigor evaluation reasoning, need to explain whether bottom-line criteria is violated and specific logic analysis, including assessment of analytical framework, reasoning process, and causal relationships>"
}}

JSON:
"""

# Build default template from prompts
DEFAULT_FINANCIAL_LOGIC_TEMPLATE = PromptTemplate(
    messages={
        LanguageEnum.EN: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_LOGIC_PROMPT_EN),
            ),
        ],
        LanguageEnum.ZH: [
            ChatMessage(
                role="user",
                content=textwrap.dedent(FINANCIAL_LOGIC_PROMPT_ZH),
            ),
        ],
    },
)


# Pydantic models for structured LLM output
class FinancialLogicEvaluationOutput(BaseModel):
    """Structured output model for financial logic evaluation LLM response."""

    score: int = Field(ge=1, le=5, description="逻辑严谨性得分 (1-5)")
    reason: str = Field(default="", description="逻辑严谨性判断原因")


def _normalize_score(score: Union[int, float]) -> float:
    """
    Normalize a 1-5 integer score to 0-1 continuous scale.

    Mapping:
    - 1 -> 0.0
    - 2 -> 0.25
    - 3 -> 0.5
    - 4 -> 0.75
    - 5 -> 1.0

    Args:
        score: Integer score from 1 to 5

    Returns:
        float: Normalized score from 0.0 to 1.0
    """
    # Clamp score to valid range [1, 5]
    score = max(1, min(5, float(score)))
    # Normalize: (score - 1) / 4
    return (score - 1) / 4.0


class FinancialReportLogicGrader(LLMGrader):
    """
    Financial deep research report logical rigor evaluation grader.

    Evaluates whether a financial research agent's report follows financial
    principles and economic logic with clear causal relationships.

    This grader first checks for bottom-line criteria violations, then evaluates
    logical rigor on a 1-5 scale, which is normalized to 0-1 for the final score.

    Attributes:
        name: Grader name
        model: ChatModelBase instance for evaluation
        language: Language for evaluation prompts

    Example:
        >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
        >>> api = OpenAIChatModel(api_key="...", model="gpt-4o")
        >>> grader = FinancialReportLogicGrader(model=api)
        >>> result = await grader.aevaluate(
        ...     messages=[
        ...         {"role": "user", "content": "分析某公司财报"},
        ...         {"role": "assistant", "content": "详细的财务分析报告..."}
        ...     ],
        ...     chat_date="2024-01-01"
        ... )
        >>> print(f"Score: {result.score}")  # normalized 0-1 score
    """

    def __init__(
        self,
        model: Union[BaseChatModel, dict],
        template: Optional[PromptTemplate] = DEFAULT_FINANCIAL_LOGIC_TEMPLATE,
        language: LanguageEnum = LanguageEnum.ZH,
    ):
        """
        Initialize the FinancialLogicGrader.

        Args:
            model (Union[BaseChatModel, dict]): The chat model to use for evaluation.
                Can be either a BaseChatModel instance or a dictionary configuration.
            template (Optional[PromptTemplate]): The prompt template for logic evaluation.
                Defaults to DEFAULT_FINANCIAL_LOGIC_TEMPLATE.
            language (LanguageEnum): Language for the evaluation prompt.
                Defaults to LanguageEnum.ZH (Chinese).

        Example:
            >>> from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
            >>> model = OpenAIChatModel(api_key="...", model="gpt-4o")
            >>> grader = FinancialLogicGrader(model=model)
        """
        super().__init__(
            name="financial_report_logic",
            mode=GraderMode.POINTWISE,
            description="Financial deep research report logical rigor evaluation",
            model=model,
            template=template,
            language=language,
            structured_model=FinancialLogicEvaluationOutput,
        )

    def _extract_query_and_answer_from_messages(
        self,
        messages: List[Dict[str, Any]],
    ) -> tuple[str, str]:
        """
        Extract user query and answer from messages.

        Args:
            messages: List of message dicts (standard format).

        Returns:
            Tuple of (query, answer)
        """
        # Filter out system messages and unwrap nested structure
        messages = [msg.get("message", msg) for msg in messages]
        non_system_messages = [msg for msg in messages if msg.get("role", "") != "system"]

        if not non_system_messages:
            return "", ""

        # Extract user query (first non-system user message)
        query = ""
        if non_system_messages[0].get("role", "") == "user":
            query = non_system_messages[0].get("content", "")

        # Extract answer (last assistant message content)
        answer = ""
        for msg in reversed(non_system_messages):
            if msg.get("role", "") == "assistant" and msg.get("content", ""):
                answer = msg.get("content", "")
                break

        return query, answer

    async def aevaluate(
        self,
        messages: List[Dict[str, Any]],
        chat_date: Optional[str] = None,
    ) -> GraderScore:
        """
        Evaluate financial research report logical rigor.

        The evaluation first checks for bottom-line criteria violations, then scores
        logical rigor on a 1-5 scale, which is normalized to 0-1 for the final score.

        Args:
            messages: List of messages (standard format, including system, user, assistant)
                The "message" key for message can be optional.

            chat_date (Optional[str]): Date of report generation. If not provided,
                uses current system date (YYYY-MM-DD format)

        Returns:
            GraderScore: Logical rigor evaluation score for the report (normalized 0.0-1.0)
                - score: Normalized logic score (0.0-1.0)
                - reason: Logical rigor evaluation reasoning
                - metadata: Contains raw_score (1-5)

        Example:
            >>> result = await grader.aevaluate(
            ...     messages=[
            ...         {"role": "user", "content": "分析某公司的财务状况"},
            ...         {"role": "assistant", "content": "详细的财务分析报告..."}
            ...     ],
            ...     chat_date="2024-01-01"
            ... )
            >>> print(f"Logic Score: {result.score}")
        """
        # Extract query and answer from messages
        query, answer = self._extract_query_and_answer_from_messages(messages)

        # Use provided chat_date if available, otherwise use current system date
        if chat_date is None:
            chat_date = datetime.now().strftime("%Y-%m-%d")

        if not query or not answer:
            logger.warning("Empty query or answer, returning error")
            return GraderError(name=self.name, error="Empty query or answer")

        try:
            # Call parent evaluation with formatted parameters
            result = await super().aevaluate(
                query=query,
                answer=answer,
                chat_date=chat_date,
            )

            # Normalize score from 1-5 to 0-1
            normalized_score = _normalize_score(result.score)

            return GraderScore(
                name=self.name,
                score=normalized_score,
                reason=result.reason,
                metadata=result.metadata,
            )

        except Exception as e:
            logger.error(f"Error evaluating {self.name}: {e}")
            return GraderError(name=self.name, error=str(e))
