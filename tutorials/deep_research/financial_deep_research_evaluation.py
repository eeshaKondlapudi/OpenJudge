"""
Deep Research Agent Evaluation Tutorial

This tutorial demonstrates how to evaluate a deep research agent using multiple graders
that assess different aspects of the agent's performance:

1. Financial Report Resolution: Evaluates report quality and problem resolution
2. Financial Trajectory Faithfulness: Checks factual accuracy against search results
3. Rubrics-Based Performance: Evaluates against custom criteria
4. Trajectory Comprehensive: Assesses step-by-step contribution
5. Observation Information Gain: Measures information redundancy
6. Action Loop Detection: Detects repetitive actions

All graders are aggregated using GradingRunner for concurrent evaluation.
"""

import asyncio
from typing import Any, Dict, List

from tutorials.deep_research.graders.financial_report_resolution import FinancialReportResolutionGrader
from tutorials.deep_research.graders.financial_trajectory_faithfulness import FinancialTrajectoryFaithfulGrader
from rm_gallery.core.graders.agent.trajectory.rubrics_based_trajectory_performance import RubricsBasedTrajectoryPerformance
from rm_gallery.core.graders.agent.observation.observation_information_gain import ObservationInformationGainGrader
from rm_gallery.core.graders.agent.action.action_loop import ActionLoopDetectionGrader
from rm_gallery.core.graders.agent.trajectory.trajectory_comprehensive import TrajectoryComprehensiveGrader
from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
from rm_gallery.core.models.schema.prompt_template import LanguageEnum
from rm_gallery.core.runner.grading_runner import GraderConfig, GradingRunner


def create_sample_data() -> List[Dict[str, Any]]:
    """Create sample evaluation data for demonstration.
    
    Returns:
        List of evaluation samples with agent trajectories
    """
    return [
        {
            "messages": [
                {"role": "user", "content": "åˆ†æžè´µå·žèŒ…å°2025å¹´ä¸ŠåŠå¹´çš„è´¢åŠ¡è¡¨çŽ°"},
                {
                    "role": "assistant",
                    "tool_calls": [
                        {
                            "function": {
                                "name": "search_financial_data",
                                "arguments": '{"company": "è´µå·žèŒ…å°", "period": "2025H1"}',
                            }
                        }
                    ],
                },
                {
                    "role": "tool",
                    "content": "æˆªè‡³2025å¹´6æœˆ30æ—¥ï¼Œè´µå·žèŒ…å°å®žçŽ°è¥ä¸šæ”¶å…¥789.5äº¿å…ƒï¼ŒåŒæ¯”å¢žé•¿15.2%ï¼›å‡€åˆ©æ¶¦454.03äº¿å…ƒï¼ŒåŒæ¯”å¢žé•¿18.5%ã€‚",
                },
                {
                    "role": "assistant",
                    "content": "æ ¹æ®è´¢åŠ¡æ•°æ®ï¼Œè´µå·žèŒ…å°2025å¹´ä¸ŠåŠå¹´è¡¨çŽ°ä¼˜å¼‚ã€‚è¥ä¸šæ”¶å…¥è¾¾789.5äº¿å…ƒï¼ŒåŒæ¯”å¢žé•¿15.2%ï¼›å‡€åˆ©æ¶¦454.03äº¿å…ƒï¼ŒåŒæ¯”å¢žé•¿18.5%ã€‚å…¬å¸ç›ˆåˆ©èƒ½åŠ›æŒç»­å¢žå¼ºï¼Œä¸šç»©å¢žé•¿ç¨³å¥ã€‚",
                },
            ],
            "chat_date": "2025-07-15",
            "rubrics": [
                {
                    "dimension": "æ•°æ®å‡†ç¡®æ€§",
                    "description": "æŠ¥å‘Šä¸­çš„æ•°æ®æ˜¯å¦å‡†ç¡®æ— è¯¯",
                    "check_points": ["è¥ä¸šæ”¶å…¥æ•°æ®æ­£ç¡®", "å‡€åˆ©æ¶¦æ•°æ®æ­£ç¡®", "å¢žé•¿çŽ‡è®¡ç®—å‡†ç¡®"],
                },
                {
                    "dimension": "åˆ†æžå®Œæ•´æ€§",
                    "description": "åˆ†æžæ˜¯å¦å…¨é¢è¦†ç›–å…³é”®æŒ‡æ ‡",
                    "check_points": ["åŒ…å«æ”¶å…¥åˆ†æž", "åŒ…å«åˆ©æ¶¦åˆ†æž", "åŒ…å«åŒæ¯”å¢žé•¿"],
                },
            ],
        }
    ]


def create_grader_configs(model: OpenAIChatModel, language: LanguageEnum = LanguageEnum.ZH) -> Dict[str, GraderConfig]:
    """Create grader configurations with appropriate mappers.
    
    Args:
        model: LLM model for evaluation
        language: Language for evaluation prompts
        
    Returns:
        Dictionary of grader configurations
    """
    return {
        # Report quality evaluation - requires messages and chat_date
        "report_resolution": GraderConfig(
            grader=FinancialReportResolutionGrader(model=model, language=language),
            mapper=lambda data: {"messages": data["messages"], "chat_date": data.get("chat_date")},
        ),
        # Factual accuracy evaluation - requires messages only
        "trajectory_faithfulness": GraderConfig(
            grader=FinancialTrajectoryFaithfulGrader(model=model, language=language),
            mapper=lambda data: {"messages": data["messages"]},
        ),
        # Rubrics-based evaluation - requires messages and rubrics
        "rubrics_performance": GraderConfig(
            grader=RubricsBasedTrajectoryPerformance(model=model, language=language),
            mapper=lambda data: {"messages": data["messages"], "rubrics": data.get("rubrics", [])},
        ),
        # Comprehensive trajectory evaluation - requires messages only
        "trajectory_comprehensive": GraderConfig(
            grader=TrajectoryComprehensiveGrader(model=model, language=language),
            mapper=lambda data: {"messages": data["messages"]},
        ),
        # Information gain evaluation - requires messages only
        "information_gain": GraderConfig(
            grader=ObservationInformationGainGrader(similarity_threshold=0.5),
            mapper=lambda data: {"messages": data["messages"]},
        ),
        # Action loop detection - requires messages only
        "action_loop": GraderConfig(
            grader=ActionLoopDetectionGrader(similarity_threshold=1.0),
            mapper=lambda data: {"messages": data["messages"]},
        ),
    }


async def main():
    """Main evaluation workflow."""
    # Initialize LLM model
    model = OpenAIChatModel(
        model="qwen3-max",
        temperature=0.0,
    )

    # Create grader configurations
    grader_configs = create_grader_configs(model, language=LanguageEnum.ZH)

    # Initialize runner with concurrent execution
    runner = GradingRunner(
        grader_configs=grader_configs,
        max_concurrency=6,  # Run all 6 graders concurrently
        show_progress=True,
    )

    # Prepare evaluation data
    dataset = create_sample_data()

    # Run evaluation
    print("Starting deep research agent evaluation...")
    results = await runner.arun(dataset)

    # Display results
    print("\n" + "=" * 80)
    print("EVALUATION RESULTS")
    print("=" * 80)

    for grader_name, grader_results in results.items():
        print(f"\n{'â”€' * 80}")
        print(f"ðŸ“Š {grader_name.upper()}")
        print(f"{'â”€' * 80}")

        for i, result in enumerate(grader_results):
            print(f"\nSample {i + 1}:")
            if hasattr(result, "score"):
                print(f"  Score: {result.score:.4f}")
                print(f"  Reason: {result.reason[:200]}...")
                if hasattr(result, "metadata") and result.metadata:
                    print(f"  Metadata: {list(result.metadata.keys())}")
            else:
                print(f"  Error: {result.error}")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    asyncio.run(main())

